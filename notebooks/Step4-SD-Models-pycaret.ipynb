{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6cd288",
   "metadata": {},
   "source": [
    "# Step 4: Create models with SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00973f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import sys \n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from pycaret.classification import ClassificationExperiment\n",
    "from pycaret.containers.models.classification import get_all_model_containers\n",
    "\n",
    "from sklearn.metrics import (classification_report, \n",
    "                             roc_auc_score, \n",
    "                             matthews_corrcoef,\n",
    "                             cohen_kappa_score)\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_validate\n",
    "\n",
    "# Import help methods\n",
    "sys.path.append('../src')\n",
    "from utils import (getExperimentConfig, \n",
    "                   getPicklesFromDir, \n",
    "                   run_pycaret_setup, \n",
    "                   translate_model_name,\n",
    "                   get_synthetic_filepaths_from_original_data_id,\n",
    "                   convert_and_clean_dict)\n",
    "\n",
    "from tuning_grids import Grids\n",
    "\n",
    "# Get global variables for the experiment\n",
    "config = getExperimentConfig()\n",
    "# Get folders\n",
    "folders = config['folders']\n",
    "# Load dataset specific settings (from the real-data)\n",
    "dataset_settings = getPicklesFromDir(folders['settings_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9310233f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training for D00.\n",
      "Starting training for synthetic dataset SD00Q10_0.\n",
      "Starting training for synthetic dataset SD00Q10_1.\n",
      "Starting training for synthetic dataset SD00Q10_2.\n",
      "Starting training for synthetic dataset SD00Q10_3.\n",
      "Starting training for synthetic dataset SD00Q10_4.\n",
      "Starting training for synthetic dataset SD00Q10_5.\n",
      "Starting training for synthetic dataset SD00Q10_6.\n",
      "Starting training for synthetic dataset SD00Q10_7.\n",
      "Starting training for synthetic dataset SD00Q10_8.\n",
      "Starting training for synthetic dataset SD00Q10_9.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# read performance data from Step 2\n",
    "model_performance_df = pd.read_csv(folders['model_perf_filepath'])\n",
    "# Specify the metrics to sort by for choosing best model\n",
    "# Choose the target metric when tuning the models\n",
    "sort_by = config['clf']['tuning_param']['optimize']\n",
    "\n",
    "run_dataset = config['run_dataset']\n",
    "\n",
    "for settings in dataset_settings:\n",
    "        \n",
    "    if run_dataset is not None and settings['meta']['id'] not in run_dataset:\n",
    "        continue\n",
    "    print(f\"Starting model training for {settings['meta']['id']}.\")    \n",
    "    settings['setup_param']['fold'] = config['clf']['cv_folds']\n",
    "    # update system_log name\n",
    "    settings['setup_param']['system_log'] = folders['log_dir']+\"Step4_SD\"\n",
    "    # disable saving train-test split data (to save space)\n",
    "    settings['setup_param']['log_data'] = False\n",
    "    target_label=settings['meta']['target']\n",
    "    \n",
    "    # Get experiment logging\n",
    "    experiment_name = f\"{settings['meta']['id']}-{settings['meta']['name']}\"\n",
    "     \n",
    "    # load original dataset\n",
    "    cols_dtype=None\n",
    "    if settings['meta']['cols_dtype'] != None:\n",
    "        cols_dtype = settings['meta']['cols_dtype']\n",
    "        \n",
    "    original_data = pd.read_csv(f\"{folders['real_dir']}{settings['meta']['filename']}\", dtype=cols_dtype)\n",
    "    \n",
    "    logg_tags = {\n",
    "        'Trained on': 'synthetic',\n",
    "        'Tuned on': 'original',\n",
    "    }\n",
    "    \n",
    "    # Filter the DataFrame based on the Dataset id and sort by specified column\n",
    "    filtered_df = model_performance_df[model_performance_df[\"Dataset id\"] == settings['meta']['id']]\n",
    "    \n",
    "\n",
    "    synthetic_datasets = get_synthetic_filepaths_from_original_data_id(settings['meta']['id'])\n",
    "\n",
    "    for sd_filename in synthetic_datasets:\n",
    "        sd_id = os.path.splitext(sd_filename)[0]\n",
    "        print(f\"Starting training for synthetic dataset {sd_id}.\")\n",
    "        quality = re.findall('Q\\d+', sd_id)[0]\n",
    "        sd_path = folders['sd_dir']+sd_filename\n",
    "        settings['setup_param']['verbose'] = False\n",
    "\n",
    "        #retrieve test data: sample from original data\n",
    "        train_size = settings['setup_param']['train_size']\n",
    "        test_data = original_data.groupby(\n",
    "            target_label, group_keys=False).apply(\n",
    "                lambda x: x.sample(int(np.rint(train_size*len(x))))).sample(frac=1).reset_index(drop=True)\n",
    "        settings['setup_param']['test_data'] = test_data\n",
    "        settings['setup_param']['index'] = False\n",
    "        s = run_pycaret_setup(sd_path, settings['setup_param'], meta=settings['meta'])\n",
    "\n",
    "        for _, row in filtered_df.iterrows():\n",
    "        \n",
    "            ml_model = row.model\n",
    "                  \n",
    "            # Add custom tags to the logg, defining dataset type, and Id\n",
    "            logg_tags = {\n",
    "                'Dataset id': sd_id,\n",
    "                'model': ml_model,\n",
    "                'Quality': quality,\n",
    "                'Trained on': 'synthetic',\n",
    "                'Tuned on': 'synthetic',\n",
    "                'SDG': sd_id.split(\"_\")[0],\n",
    "            }\n",
    "            # Check if the setup has already been evaluated\n",
    "            row_exists = ((model_performance_df['Dataset id'] == logg_tags['Dataset id']) & \n",
    "                          (model_performance_df['model'] == logg_tags['model']) &\n",
    "                          (model_performance_df['Tuned on'] == logg_tags['Tuned on']) &\n",
    "                          (model_performance_df['Trained on'] == logg_tags['Trained on'])).any()\n",
    "        \n",
    "            if not row_exists:\n",
    "                print(f\"Training {ml_model}...\", end=\"\")             \n",
    "                # train the model on synthetic data\n",
    "                model = s.create_model(ml_model)  \n",
    "                print(\"done.\")\n",
    "                print(f\"Tuning {ml_model}...\", end=\"\")\n",
    "                # Get tuning grid\n",
    "                tune_grid = Grids.get_tuning_grid(ml_model)\n",
    "                # Is buggy, use default tuning by pycaret\n",
    "                model = s.tune_model(model) #, custom_grid=tune_grid, **config['clf']['tuning_param'])\n",
    "                print(\"done.\")\n",
    "                # get validation results\n",
    "                val_df = s.pull()\n",
    "                val_score = {}\n",
    "                val_score['val_Accuracy'] = val_df['Accuracy']['Mean']\n",
    "                val_score['val_F1'] = val_df['F1']['Mean']\n",
    "\n",
    "                metrics_list = []\n",
    "                #run monte carlo stratified cross-validation using StratifiedShuffelSplit\n",
    "                x_test = s.get_config(\"X_test_transformed\")\n",
    "                y_test = s.get_config(\"y_test_transformed\")\n",
    "                \n",
    "\n",
    "                #x_test_transformed = s.pipeline.transform(x_test)\n",
    "                # Rearrange the column order in the same order as the train data\n",
    "                #x_test = x_test[s.X_train_transformed.columns]\n",
    "                \n",
    "                y_pred = model.predict(x_test)\n",
    "\n",
    "                metrics =  classification_report(y_true=y_test, y_pred=y_pred, output_dict=True, digits=4)\n",
    "                test_metrics = {\n",
    "                    \"Accuracy\": metrics['accuracy'],\n",
    "                    \"Precision_macro\": metrics['macro avg']['precision'],\n",
    "                    \"Recall_macro\": metrics['macro avg']['recall'],\n",
    "                    \"F1_macro\": metrics['macro avg']['f1-score'],\n",
    "                    \"Precision_weighted\": metrics['weighted avg']['precision'],\n",
    "                    \"Recall_weighted\": metrics['weighted avg']['recall'],\n",
    "                    \"F1_weighted\": metrics['weighted avg']['f1-score'],\n",
    "                    \"MCC\": matthews_corrcoef(y_true=y_test, y_pred=y_pred),\n",
    "                    \"Kappa\": cohen_kappa_score(y1=y_test, y2=y_pred)\n",
    "                }\n",
    "                metrics_list.append(test_metrics)\n",
    "\n",
    "                # Convert the list of dictionaries to a DataFrame\n",
    "                metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "                # Calculate the average of each column\n",
    "                average_metrics = metrics_df.mean()\n",
    "                # save results\n",
    "                performance_row = {**logg_tags, **test_metrics}\n",
    "                performance_row['Params'] = model.get_params()\n",
    "                # model_performance_df.append(performance_row, ignore_index=True)\n",
    "                model_performance_df = pd.concat([model_performance_df, pd.DataFrame(performance_row)], ignore_index=True)\n",
    "                ########### End test hyper-param ###########        \n",
    "\n",
    "\n",
    "        # update model performance to csv after each sd_id\n",
    "        model_performance_df.to_csv(folders['model_perf_filepath'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3af05-296f-455d-8a76-bd4652ae7ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "163px",
    "width": "322px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "328px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
