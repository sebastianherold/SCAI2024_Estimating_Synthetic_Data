{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f9411b-734f-4616-aab0-a36163b7fcab",
   "metadata": {},
   "source": [
    "# Step 3: SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81739df5-f21f-4ee5-8109-992bc7c8519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports for the section\n",
    "import sdv\n",
    "from sdv.single_table import CTGANSynthesizer as CTGAN\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "import pickle\n",
    "import pandas as pd \n",
    "import os \n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "sys.path.append('../src')\n",
    "from utils import (getPicklesFromDir, \n",
    "                   getExperimentConfig, \n",
    "                   extract_loss_info_from_stdout, \n",
    "                   create_loss_plot)\n",
    "\n",
    "# Get global experiment settings\n",
    "config = getExperimentConfig()\n",
    "# Get folders\n",
    "folders = config['folders']\n",
    "# Get dataset specific settings\n",
    "dataset_settings = getPicklesFromDir(folders['settings_dir'])\n",
    "print(f\"SDV version installed: {sdv.version.public}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8070cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_stdout(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Save the original stdout\n",
    "        original_stdout = sys.stdout\n",
    "\n",
    "        # Create a new StringIO object to temporarily redirect stdout\n",
    "        sys.stdout = StringIO()\n",
    "        \n",
    "        # Call the original function and get its output\n",
    "        func_output = func(*args, **kwargs)\n",
    "\n",
    "        # Retrieve the captured stdout\n",
    "        captured_stdout = sys.stdout.getvalue()\n",
    "        \n",
    "        # Close IO and restore the original stdout\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = original_stdout\n",
    "         # Return both the function output and the captured stdout\n",
    "        return func_output, captured_stdout\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "@capture_stdout\n",
    "def train_sdg_model(model, data, sdg_name):\n",
    "    print(\"#START#\")\n",
    "    print(sdg_name)\n",
    "    model.fit(data)\n",
    "    print(\"#END#\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a99e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify datasets by Id, if None, all is run\n",
    "run_dataset = config['run_dataset']\n",
    "\n",
    "# get settings\n",
    "quality_params = config['ctgan_param']['quality_params']\n",
    "sd_size_factor = config['ctgan_param']['sd_size_factor']\n",
    "num_SD = config['ctgan_param']['num_sd']\n",
    "\n",
    "\n",
    "# run SDG generation\n",
    "# for each dataset specific settings\n",
    "for s_index, settings in enumerate(dataset_settings):\n",
    "    \n",
    "    if run_dataset is not None and settings['meta']['id'] not in run_dataset:\n",
    "        continue\n",
    "    \n",
    "    metadata = SingleTableMetadata().load_from_json(settings['meta']['meta_filepath'])\n",
    "    experiment_name = f\"{settings['meta']['id']}-SDG\"\n",
    "    \n",
    "    # load original dataset\n",
    "    cols_dtype=None\n",
    "    if 'cols_dtype' in settings['meta']:\n",
    "        cols_dtyped = settings['meta']['cols_dtype']\n",
    "        \n",
    "    original_data = pd.read_csv(f\"{folders['real_dir']}{settings['meta']['filename']}\", dtype=cols_dtype)\n",
    "    \n",
    "    # get the size to generate the synthetic data\n",
    "    original_data_size = len(original_data)\n",
    "    sd_size = original_data_size * sd_size_factor\n",
    "    \n",
    "    logg_tags = {'Source': settings['meta']['id']}\n",
    "    \n",
    "    # loop through the different quality parameters for the SDG\n",
    "    for quality in quality_params:\n",
    "        sdg_name = f\"S{settings['meta']['id']}{quality}\"\n",
    "        logg_tags['Quality'] = quality\n",
    "                \n",
    "        # Get path to save the artifacts, relative to notebooks dir\n",
    "        artifact_path='../data/result/SDG/'\n",
    "        # creates model with sdg_param and quality_param as parameters\n",
    "        model = CTGAN(metadata=metadata, **quality_params[quality])\n",
    "        \n",
    "        if 'sdg_constraints' in settings['meta']:\n",
    "            model.add_constraints(constraints=settings['meta']['sdg_constraints'])\n",
    "            \n",
    "        print(f\"Start fitting {sdg_name}...\")\n",
    "        model.fit(original_data)\n",
    "        #model, stdout_loss = train_sdg_model(model, original_data, sdg_name)\n",
    "        # extract loss, create loss plot and save it\n",
    "        print(\"...done.\")\n",
    "        #loss_dict = extract_loss_info_from_stdout(stdout_loss)\n",
    "        #create and save loss plot\n",
    "        print(\"Saving loss plot...\", end=\"\")\n",
    "        fig = model.get_loss_values_plot()\n",
    "        fig_path = f\"{artifact_path}/{sdg_name}_loss.html\"\n",
    "        fig.write_html(fig_path)\n",
    "        print(\"done.\")\n",
    "        #save loss data\n",
    "        print(\"Saving loss data...\", end=\"\")\n",
    "        loss_df_path = f\"{artifact_path}/{sdg_name}_loss.csv\"\n",
    "        model.get_loss_values().to_csv(loss_df_path, index=False)        \n",
    "        print(\"done.\")\n",
    "        # saves the SDG model using cloudpickle\n",
    "        print(\"Saving model...\", end=\"\") \n",
    "        model_path = f\"{folders['SDGs_dir']}/{sdg_name}.pkl\"\n",
    "        model.save(model_path)\n",
    "        print(\"done.\")\n",
    "        # create num_SD SDGs and synthetic datasets for validating results\n",
    "        for itr in range(num_SD):\n",
    "            print(f\"Generating synthetic dataset #{itr}...\", end=\"\")\n",
    "            # creates Synthetic dataset name, using datset id, quality key, and itr number \n",
    "            # e.g. SD1Q1_2 means SDG trained on datset D1 with quality Q1 and copy num 2\n",
    "            SD_name = f\"S{settings['meta']['id']}{quality}_{str(itr)}\"\n",
    "            \n",
    "            # relative file path for the synthetic dataset\n",
    "            sd_path = f\"{folders['sd_dir']}{SD_name}.csv\"\n",
    "            \n",
    "            # generate synthetic data\n",
    "            synthetic_data = model.sample(num_rows=sd_size)\n",
    "            \n",
    "            # save the synthetic dataset\n",
    "            synthetic_data.to_csv(sd_path, index=False)\n",
    "            print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aca606-0d16-48a2-89f9-4029d2873beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "163px",
    "width": "322px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "328px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
