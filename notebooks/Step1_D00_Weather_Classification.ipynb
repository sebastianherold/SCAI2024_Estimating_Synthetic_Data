{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2262fa3c",
   "metadata": {},
   "source": [
    "# Step 1: Data preperation, D00 Weather Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385f05a-34b6-4e23-88bd-aaf61d5b1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "# Display all available datsets\n",
    "for dirname, _, filenames in os.walk('../data/raw/'):\n",
    "    for filename in filenames:\n",
    "        display(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d340b",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('../src')\n",
    "from utils import getExperimentConfig\n",
    "\n",
    "# Get global experiment settings\n",
    "config = getExperimentConfig()\n",
    "folders = config['folders']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24021710",
   "metadata": {},
   "source": [
    "In this section the data will be examined for selecting the preprocessing and model of the original dataset. This pipeline of preprocessing will then be save for executing on the respective synthetic dataset.\n",
    "\n",
    "\n",
    "This section will be done independently for each dataset that will be explored, with the hopes that rest of the steps of the experiment can be automized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ae13c-3e7a-4db1-bcf5-23511fc4ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Select the dataset id\n",
    "data_id = 'D00'\n",
    "data_name = 'weather'\n",
    "data_filename = \"weather.csv\"\n",
    "data = pd.read_csv(f\"{folders['raw_dir']}weather_classification.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc312c12-16c2-477e-a70e-497f1b9518ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32014532-f04b-4b0e-8f4d-266a2f004857",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfileReport(data, minimal=True, explorative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a165e1-13a6-486d-beac-4b7aaec44a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Null values: ===\\n\")\n",
    "display(data.isnull().sum())\n",
    "print(\"\\n=== Data types: === \\n\")\n",
    "display(data.info(verbose=True, memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e094bf0",
   "metadata": {},
   "source": [
    "No missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f6372",
   "metadata": {},
   "source": [
    "### Define metadata for the dataset\n",
    "The following cells in this section is for defining the dataset specific settings that are needed to run the following experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b650870f",
   "metadata": {},
   "source": [
    "> NOTICE:\n",
    "*The meta dictionary gets updated in Step 3: SDG, where metadata about each synthetic data that is generated on the respective real data. Data is appended to 'sd_meta_list' key.\n",
    "This is then saved over the current settings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e80b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata for the SDG\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16dbc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metadata & validate\n",
    "display(metadata)\n",
    "\n",
    "display(metadata.validate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf930693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# metadata.set_primary_key(column_name='PassengerId')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7912a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dtypes)\n",
    "# define dtypes for dataframe\n",
    "cols_dtype = {\n",
    "    'Temperature': 'Int32',\n",
    "    'Humidity': 'Float32',\n",
    "    'Wind Speed': 'Float32',\n",
    "    'Precipitation (%)': 'Int32',\n",
    "    'Cloud Cover': 'category',\n",
    "    'Atmospheric Pressure': 'Float32',\n",
    "    'UV Index': 'UInt8',\n",
    "    'Season': 'category',\n",
    "    'Visibility (km)': 'Float32',\n",
    "    'Location': 'category',\n",
    "    'Weather Type': 'category'\n",
    "}\n",
    "data.info()\n",
    "t = pd.read_csv(f\"{folders['real_dir']}{data_id}-{data_filename}\", dtype=cols_dtype)\n",
    "\n",
    "data.info(verbose=True, memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe82e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Define dataset id and save metadata\n",
    "\n",
    "meta_filepath = f\"{folders['meta_dir']}{data_id}\"\n",
    "\n",
    "try:\n",
    "    metadata.save_to_json(meta_filepath)\n",
    "\n",
    "except:\n",
    "    print(f\"File {meta_filepath} already exits and has been replaced.\")\n",
    "    os.remove(meta_filepath)\n",
    "    metadata.save_to_json(meta_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset meta data for the setup parameters in pycaret\n",
    "# use this to avoid needing to save the whole dataset in a pickle object\n",
    "\n",
    "# use the parameters to read the data from csv into the setup, e.g.\n",
    "meta = {\n",
    "    # Generall\n",
    "    'name':     data_name,\n",
    "    'id':       data_id,\n",
    "    'filename': f\"{data_id}-{data_filename}\",\n",
    "    'cols_dtype': cols_dtype,\n",
    "    \n",
    "    # Pycaret\n",
    "    'target': 'Weather Type',\n",
    "    \n",
    "    'ordinal_features': None,\n",
    "    #'ordinal_features': {\n",
    "   #     'UV Index': range(15)\n",
    "   # },         # (dict) default=None, the columns with ordinal values, \n",
    "                                      #   e.g. {'column name': ['low', 'med', 'high']}\n",
    "    \n",
    "    'numeric_features': [\n",
    "        'UV Index',\n",
    "        'Temperature',\n",
    "        'Humidity',\n",
    "        'Wind Speed',\n",
    "        'Precipitation (%)',\n",
    "        'Atmospheric Pressure',\n",
    "        'Visibility (km)'\n",
    "    ],         # (string[]) default = None, columns that contain \n",
    "                                      # numeric values\n",
    "    \n",
    "    'text_features': None,            # (string[]) default = None, columns that contain text\n",
    "    \n",
    "    'categorical_features': [\n",
    "        'Cloud Cover',\n",
    "        'Season',\n",
    "        'Location'\n",
    "    ],     # (string[]) default=None, if inferred types are not \n",
    "\n",
    "    'meta_filepath': meta_filepath,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0a1b8",
   "metadata": {},
   "source": [
    "> Note on Iterative imputation that exists in pycaret:\n",
    "*Iterative imputation is a imputation method that for each feature, sets up a model to predict the missing values with the rest of the features as predictors, then repeatedly does this for each feature with missing values.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf3f1b4",
   "metadata": {},
   "source": [
    "### Define setup parameters for pycaret\n",
    "Use these settings to instruct for pycaret how to preprocess the data, handle the model training and evaluation. Basically the ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb32a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the setup parameters for pycaret setup function, where the details of preprocessing is defined\n",
    "# Note: can only contain keywords that exists in the settings of the pycaret.setup()\n",
    "\n",
    "setup_param = {\n",
    "    'target': meta['target'],\n",
    "\n",
    "    ### Sampling settings ###\n",
    "    'train_size': 0.8,  # (float) default=0.7, the train test split\n",
    "    # used for training and validation\n",
    "    'fold_strategy': 'stratifiedkfold',  # (srt), default = 'stratifiedkfold',\n",
    "    'data_split_stratify': True,\n",
    "    # selects cross-validation method\n",
    "    'fold': config['clf']['cv_folds'],  # (int) default=10, the number of folds\n",
    "\n",
    "    ### Data-preparation settings ###\n",
    "\n",
    "    #### Define features (use meta) ####\n",
    "    'ordinal_features': meta['ordinal_features'],\n",
    "    'numeric_features': meta['numeric_features'],\n",
    "    'text_features': meta['text_features'],\n",
    "    'categorical_features': meta['categorical_features'],\n",
    "\n",
    "    #### Imputation methods #### \n",
    "    #Note: imputation will be performed in step 1, instead of in pycaret\n",
    "    'imputation_type': None,  # ('simple', 'iterative', None) default='simple'\n",
    "    'numeric_imputation': 'mean',  # (int, float or str) default='mean',\n",
    "                        # it's ignored if imputation_type='iterative'\n",
    "                        # alternatives:\n",
    "                        #   'drop'      : drops rows with missing values\n",
    "                        #   'mean'      : replace with mean of column\n",
    "                        #   'median'    : replace with median of column\n",
    "                        #   'mode'      : replace with mode of column\n",
    "                        #   'knn'       : replace with KNN approach\n",
    "                        #   int or float: replace with provided value\n",
    "    'categorical_imputation': 'mode',  # same as numeric, but only with 'drop', 'mode' and str\n",
    "                                       # (replace with str)\n",
    "\n",
    "    # iterative imputation is automatically ignored if imputation_type='simple' or None\n",
    "    'iterative_imputation_iters': 10,  # (int), default=5, number of iterations\n",
    "    'numeric_iterative_imputer': 'lightgbm',  # (str or sklearn estimator), default='lightgbm',\n",
    "                                             # the regression algorithm for numeric imputation\n",
    "    'categorical_iterative_imputer': 'lightgbm',  # (str or sklearn estimator), default='lightgbm'\n",
    "\n",
    "    \n",
    "    #### Text encoding ####\n",
    "    'text_features_method': 'tf-idf',  # (str), default='tf-idf', alternative 'bow'\n",
    "    'max_encoding_ohe': 25,  # (int), default=25, cat. columns with less than specified value\n",
    "                                # will be encoded with OneHotEncoding.\n",
    "    'encoding_method': None,  # (category-encoders estimator), default=None, \n",
    "                              # for cat. cols with more unique values than 'max_encoding_ohe',\n",
    "                              # if none, then default = leave_one_out.LeaveOneOutEncoder\n",
    "\n",
    "    \n",
    "    #### Feature engineering ####\n",
    "    'low_variance_threshold': None,  # (float or none), default=None, \n",
    "                                     # variance threshold for features, features\n",
    "                                     # with lower variance are discarded -- if none, keep all features.\n",
    "    'remove_multicollinearity': False, # (bool), default=False, use correlation as threshold for feature selection\n",
    "    'multicollinearity_threshold': 0.01,  # (float), default=0.9, use if setting above is true\n",
    "    \n",
    "    'bin_numeric_features': None, # (string[]), default=None, convert numeric features into categorical.\n",
    "    'remove_outliers': False,  # (bool), default=False, remove outliers using an isolation forest.\n",
    "    'outliers_method': 'iforest',  # (string), default='iforest', alternatives:\n",
    "                                    # 'iforest': sklearn's IsolationForest\n",
    "                                    # 'ee': sklearn's EllipticEnvelope\n",
    "                                    # 'lof': sklearn's LocalOutlierFactor\n",
    "    'outliers_threshold': 0.05,  # (float), default=0.05, the percentage of outliers to be removed,\n",
    "                                # is ignored when 'remove_outliers'=False.\n",
    "    'fix_imbalance': False,  # (bool) default=False, use SMOTE to fix imbalance target features,\n",
    "                                # can specify other method with 'fix_imbalance_method'\n",
    "    'fix_imbalance_method': 'SMOTE',  # (str), default='SMOTE', estimator to use\n",
    "    \n",
    "    'transformation': False,  # (bool) default=False, if true apply power transform\n",
    "                              # to make the data more Gaussian-like\n",
    "    'transformation_method': 'yeo-johnson',  # (str), default='yeo-johnson'\n",
    "    \n",
    "    'normalize': True,  # (bool) default=False, scale data\n",
    "    'normalize_method': 'zscore',  # (str) default='zscore', alt: 'minmax'\n",
    "    \n",
    "    'pca': False,  # (bool) default=False, use principal component analysis\n",
    "                   # to reduce dimensionality\n",
    "    'pca_method': 'linear',  # (str) default='linear', alt: 'kernel', 'incremental'\n",
    "    'pca_components': None,  # (int,float,str,None) default=None, if:\n",
    "                             # * None: all components are kept\n",
    "                             # * int: the absolute number of components\n",
    "                             # * float: the variance limit for explaination\n",
    "                             # * \"mle\": use  Minka's MLE to guess dimension,\n",
    "                             #          only works with pca_method='linear'\n",
    "    'feature_selection': False,  # (bool) default=False, select features based on a\n",
    "                                    # feature importance score defined by following param\n",
    "    'feature_selection_method': 'classic',  # (str) default='classic', if\n",
    "                                    # * 'univariate': use sklearn SelectKBest\n",
    "                                    # * 'classic': use sklearn SelectFromModel\n",
    "                                    # * 'sequential': use sklearn SequentialFeatureSelector\n",
    "    'feature_selection_estimator': 'lightbm',  # (str, sklearn estimator) default='lightbm',\n",
    "                                    # the choice of classifier that decides feature importance,\n",
    "                                    # where the estimator needs to have 'feature_importances'\n",
    "                                    # or 'coef_attribute' after the fitting. If none, use\n",
    "                                    # LGBClassifier\n",
    "                                    # This param. is ignored when method='univariate'\n",
    "    'n_features_to_select': 0.2,  # (int,float) default=0.2, The max number of features\n",
    "                                    # to use with feature_selection, only looks at features\n",
    "                                    # allowed (i.e. not at 'ignore_features') when counting.\n",
    "\n",
    "    ###### Backend-settings ######\n",
    "\n",
    "    ### Logging settings ###\n",
    "    ### Note: have implmented manual loggning\n",
    "    'log_experiment': False,  # choose logger, alternatives: default='mlflow', 'wandb'\n",
    "    'experiment_name': f\"{meta['id']}-{meta['name']}\",  # The experiment name, set as the id-dataset name\n",
    "    'system_log': folders['log_dir'] + meta['id'],   # system loggin, for debugging\n",
    "    \n",
    "    #'experiment_custom_tags': {'Dataset Type': 'Original', 'Dataset ID': meta['id']},  # will be changed to 'Synthetic' when using synthetic data\n",
    "    #'log_plots': False,  # (bool) default=False, if true analysis plots are saved as image files\n",
    "    #'log_data': True,  # (bool) default=Flase, log the train & test datasets as a csv file\n",
    "\n",
    "    #### Hardware settings ####\n",
    "    'n_jobs': -1, # number of jobs to run in parallel (-1 means use all available processors)\n",
    "    'use_gpu': False, # (bool or str) default=False, whether the GPU should be used for training\n",
    "\n",
    "    ### Output settings ###\n",
    "    'html': True,  # (bool) default=True, prevents runtime display of the monitor,\n",
    "                    # disable when the env doesn't support IPYTHON\n",
    "                    # Todo: for real experiment, set verbose to false, to disable output of grids\n",
    "    'verbose': True,  # (bool) default=True, print information grid?\n",
    "    'profile': False,  # (bool) default=False, if true it displays an interactive EDA report\n",
    "    'preprocess': True,  # (bool) default=True, use preprocessing methods within pycaret?\n",
    "\n",
    "    # (something wrong with this argument, deprecated?)'silent': False, #(bool) default=False, need to be True when executed in a automated setting\n",
    "    # might not need following, because I will drop the features not neede in preperation of data\n",
    "    # ignore_features = None # (string[]) default=None, list of columns to be ignored in preporcessing and training\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de98861",
   "metadata": {},
   "source": [
    "#### Define settings for the Synthetic Data Generator\n",
    "Extracts the column names, and renames fields to field_types (because of implementation issue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTICE: is deprecated, as of SDV 1.0.0\n",
    "#field_names = data.columns.to_list()\n",
    "# Define the dataset specific parameters for the sdg CTGAN()\n",
    "# Note: can only contain keywords that are accepted by CTGAN() function in sdv\n",
    "sdg_param = {\n",
    "    # Metadata on the dataset\n",
    "    #\"field_names\": field_names,\n",
    "    #\"primary_key\": \"Outcome\",\n",
    "    \n",
    "    # same data as meta_data, however, \n",
    "    #the SDG model method uses a different parameter name\n",
    "    #\"columns\": meta['meta_data']['fields'],  \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699aa66",
   "metadata": {},
   "source": [
    "### Save for next steps\n",
    "In the cell below, the dataset meta-data and the settings for preprocessing and model creation is saved as a pickle object in its respective directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375bd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine then save the objects to '../pickles/settings' directory \n",
    "import pickle\n",
    "\n",
    "data_settings = {\n",
    "    \"meta\": meta,\n",
    "    \"setup_param\": setup_param,\n",
    "    \"sdg_param\": sdg_param,\n",
    "}\n",
    "\n",
    "pickle.dump(\n",
    "    data_settings, \n",
    "    open(f\"{folders['settings_dir']}{meta['id']}-settings.pkl\", 'wb') \n",
    ")\n",
    "\n",
    "data.to_csv(f\"{folders['real_dir']}{meta['filename']}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697101b3-0780-416e-8533-b5414f8ee730",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Weather Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf68a3-1587-4730-91ad-fe4242906db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bdab4e-0774-4c7d-b068-b3d90317af4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
